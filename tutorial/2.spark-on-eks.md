## Spark 오퍼레이터 설치 ##


#### 1. 설치  ####

```
kubectl create namespace spark

helm repo add spark-operator https://kubeflow.github.io/spark-operator
helm install spark spark-operator/spark-operator \
    --namespace spark \
    --set webhook.enable=true
```

----

#### 2. PySpark 프로그램 작성 (example.py) ####

```
from pyspark.sql import SparkSession

if __name__ == "__main__":
    spark = SparkSession.builder.appName("EKS-PySpark-S3").get_session()
    
    # S3 데이터 로드 (EKS 환경에서는 s3a:// 프로토콜 사용)
    df = spark.read.csv("s3a://your-bucket-name/data.csv")
    df.show()
    
    # 처리 로직...
    spark.stop()

```

#### 3. 도커 이미지 빌드 및 푸시 ####
* PySpark 코드를 실행하려면 Spark 바이너리와 Python 환경이 포함된 이미지가 필요합니다.
* Base Image: apache/spark:3.5.0 (또는 최신 4.x 버전)
* 작성한 example.py를 이미지 내부 /opt/spark/work-dir/에 복사한 후 ECR에 푸시합니다.


#### 4. SparkApplication 실행 (YAML) ####
이제 kubectl apply로 작업을 제출합니다.

```
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: pyspark-s3-job
  namespace: spark-operator
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "your-account-id.dkr.ecr.ap-northeast-2.amazonaws.com"
  # 1. 외부 S3 경로로 변경
  mainApplicationFile: "s3a://my-spark-scripts/example.py"
  sparkVersion: "3.5.0"
  sparkConf:
    # 2. S3 접근을 위한 AWS 커넥터 설정
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.hadoop.fs.s3a.aws.credentials.provider": "com.amazonaws.auth.WebIdentityTokenFileCredentialsProvider"
  driver:
    cores: 1
    memory: "512m"
    serviceAccount: airflow-sa # S3 읽기 권한이 있는 SA 사용 필수
  executor:
    cores: 1
    instances: 2
    memory: "512m"
```
* mainApplicationFile 는 컨테이너 내부 로컬 뿐만아니라 외부 경로도 지원한다. 가장 많이 사용되는 S3와 HTTP 설정 방식은 다음과 같습니다.

#### 주의사항 ####
* Jar 파일 포함 여부: 사용하는 Spark Docker 이미지 내에 S3 연동을 위한 hadoop-aws 및 aws-java-sdk Jar 파일이 포함되어 있어야 합니다.
* 권한 설정: serviceAccount(airflow-sa)에 해당 S3 버킷과 파일에 대한 s3:GetObject 권한이 반드시 할당되어 있어야 합니다.
* 종속성 파일: 만약 메인 파일 외에 추가 파이썬 파일들(.py, .zip)이 더 필요하다면 spec.deps.pyFiles 항목에 리스트 형식으로 외부 경로를 추가할 수 있습니다.
더 자세한 설정 옵션은 Spark Operator 공식 문서(API Reference)에서 확인하실 수 있습니다.


#### 5. Airflow와 연동하기 (MLOps의 완성) ####
배우고 계신 Airflow에서 이 Spark 작업을 실행하고 싶다면, SparkKubernetesOperator를 사용하면 됩니다.
* Airflow Task 1: 위 YAML 파일을 템플릿으로 사용하여 SparkApplication 리소스 생성.
* Airflow Task 2: SparkKubernetesSensor를 사용하여 Spark 작업이 성공적으로 끝났는지 모니터링.

