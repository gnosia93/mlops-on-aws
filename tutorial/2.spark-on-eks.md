## Spark 오퍼레이터 설치 ##


#### 1. 설치  ####

```
kubectl create namespace spark

helm repo add spark-operator https://kubeflow.github.io/spark-operator
helm install spark spark-operator/spark-operator \
    --namespace spark \
    --set webhook.enable=true
```

----

#### 2. PySpark 프로그램 작성 (example.py) ####

```
from pyspark.sql import SparkSession

if __name__ == "__main__":
    spark = SparkSession.builder.appName("EKS-PySpark-S3").get_session()
    
    # S3 데이터 로드 (EKS 환경에서는 s3a:// 프로토콜 사용)
    df = spark.read.csv("s3a://your-bucket-name/data.csv")
    df.show()
    
    # 처리 로직...
    spark.stop()

```

#### 3. 커스텀 이미지 만들기 ####

S3와 같은 외부 저장소에 접근하여 PySpark 작업을 수행하려면, 단순한 Spark 이미지가 아니라 S3 커넥터(hadoop-aws)와 AWS SDK가 포함된 이미지를 사용해야 합니다. 2026년 현재 추천하는 이미지 유형은 다음과 같습니다.

* Apache Spark 공식 이미지 (apache/spark:3.5.x-python3):
가장 표준적인 선택지입니다. 단, 버전에 따라 S3 관련 라이브러리가 기본 포함되지 않았을 수 있으므로 spark.jars.packages 설정을 통해 런타임에 다운로드하거나, 이 이미지를 베이스로 직접 빌드하는 것이 권장됩니다.

* Bitnami Spark 이미지 (bitnami/spark:3.5.x):
보안 강화와 빠른 업데이트로 유명하며, K8s 환경에서 안정적으로 동작합니다. 다만 PySpark 버전 호환성을 확인해야 합니다.

* Data Mechanics Spark 이미지 (datamechanics/spark:3.5.0-hadoop-3.3.1-java-11-python-3.10-main):
S3A 커넥터와 최적화 설정이 이미 포함되어 있어, 별도의 설정 없이 s3a:// 경로를 바로 사용하기에 매우 편리합니다. 

* Amazon EMR on EKS 이미지:
AWS에서 직접 관리하는 이미지로, S3 접근 성능과 IAM Role(IRSA) 연동이 가장 뛰어납니다. ECR Public Gallery에서 public.ecr.aws/emr-on-eks/spark/emr-7.x.x 형태로 찾을 수 있습니다.


#### 커스텀 이미지 제작 (권장 방식) ####
실제 운영 환경에서는 버전 충돌 방지를 위해 공식 이미지를 베이스로 필요한 Jar를 미리 넣어두는 방식이 가장 안정적입니다. 

[dockerfile]
```
FROM apache/spark:3.5.0-python3
USER root
# S3 연동에 필요한 라이브러리 추가 (버전에 맞춰 조정 필요)
ADD repo1.maven.org /opt/spark/jars/
ADD repo1.maven.org /opt/spark/jars/
USER 185
```

#### 4. SparkApplication 실행 (YAML) ####
이제 kubectl apply로 작업을 제출합니다.

```
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: pyspark-s3-job
  namespace: spark-operator
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "your-account-id.dkr.ecr.ap-northeast-2.amazonaws.com"
  # 1. 외부 S3 경로로 변경
  mainApplicationFile: "s3a://my-spark-scripts/example.py"
  sparkVersion: "3.5.0"
  sparkConf:
    # 2. S3 접근을 위한 AWS 커넥터 설정
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.hadoop.fs.s3a.aws.credentials.provider": "com.amazonaws.auth.WebIdentityTokenFileCredentialsProvider"
  driver:
    cores: 1
    memory: "512m"
    serviceAccount: airflow-sa # S3 읽기 권한이 있는 SA 사용 필수
  executor:
    cores: 1
    instances: 2
    memory: "512m"
```
* mainApplicationFile 는 컨테이너 내부 로컬 뿐만아니라 외부 경로도 지원한다. 가장 많이 사용되는 S3와 HTTP 설정 방식은 다음과 같습니다.

#### 주의사항 ####
* Jar 파일 포함 여부: 사용하는 Spark Docker 이미지 내에 S3 연동을 위한 hadoop-aws 및 aws-java-sdk Jar 파일이 포함되어 있어야 합니다.
* 권한 설정: serviceAccount(airflow-sa)에 해당 S3 버킷과 파일에 대한 s3:GetObject 권한이 반드시 할당되어 있어야 합니다.
* 종속성 파일: 만약 메인 파일 외에 추가 파이썬 파일들(.py, .zip)이 더 필요하다면 spec.deps.pyFiles 항목에 리스트 형식으로 외부 경로를 추가할 수 있습니다.
더 자세한 설정 옵션은 Spark Operator 공식 문서(API Reference)에서 확인하실 수 있습니다.


#### 5. Airflow와 연동하기 (MLOps의 완성) ####
배우고 계신 Airflow에서 이 Spark 작업을 실행하고 싶다면, SparkKubernetesOperator를 사용하면 됩니다.
* Airflow Task 1: 위 YAML 파일을 템플릿으로 사용하여 SparkApplication 리소스 생성.
* Airflow Task 2: SparkKubernetesSensor를 사용하여 Spark 작업이 성공적으로 끝났는지 모니터링.

